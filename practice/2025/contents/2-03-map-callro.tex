\section{Дизайн-документ по реализации функционала Map по репликам}

\begin{itemize}
    \item Текущая реализация Map запроса по репликам у клиента ведет к
    дублированию, потере и повреждению данных;
    \item В vshard будут добавлены новые запросы: \texttt{map\_callro},
        \texttt{map\_callre}, \texttt{map\_callbro}, \texttt{map\_callbre};
    \item \texttt{map\_*} запросы (даже те, которые выполняются на репликах)
        замедляют ребалансировку. Пользователь может регулировать распределение
        между map запросами и перевозом бакетов с использованием опций
        sched\_ref\_quota, sched\_move\_quota.
\end{itemize}

\subsection{Мотивация}

В бизнес-логике клиента очень много map-запросов, однако \texttt{vshard} на
данный момент не предлагает никакого решения для совершения map-запросов по
репликам, вследствии чего клиент вынужден писать собственные решения с
использованием \texttt{vshard.router.routeall}, которые выполняются с рисками
нарушения консистентности и целостности получаемых данных (дублирование и
потеря).

Базово сейчас \texttt{map\_callro} у клиента выглядит так (еще есть логика на
пропуск репликасетов из \texttt{routeall} при определенных условиях):

\begin{verbatim}
for _, rs in pairs(vshard.router.routeall()) do
    rs:callbro('box.space.card:select()', nil, {timeout = <number>})
end
\end{verbatim}

Это крайне небезопасно:

\begin{enumerate}
\item \textbf{Дублирование данных} - бакет может находиться в процессе
    пересылки (\texttt{state = SENDING}) или уже даже быть послан
        (\texttt{state = SENT}). Данные не удаляются во время пересылки, только
        после ее полного окончания, поэтому когда бакет переезжает данные есть
        на двух шардах. При \texttt{select} по спейсу получим дублирование
        данных. Даже если бакет полностью переехал, дублирование возможно:
        пришли на первый репликасет, там данные еще есть, пока идем до второго,
        они переехали, пришли на второй, там эти же данные уже есть. Если бакет
        в процессе удаления, то можно прочитать часть бакета, а полностью он
        будет только там, куда уехал, снова дублирование.

\item \textbf{Потеря данных} - мы идем на первый репликасет, данных там пока
    нет, нам вернули результат. Пока идем до следующего, он успевает переслать
        бакет на первый репликасет (полностью переслать, с удалением данных).
        Мы дошли до него, данных там тоже не будет. Map-запрос потерял данные.

\item \textbf{"Повреждение данных"} - можно прочитать неконсистентное состояние
    данных (\texttt{state = GARBAGE}). Допустим пользователь добавлял в один
        бакет в одной транзакции кортежи 1, 2, 3, 4, 5. Бакет уехал. Кортежи 1, 2,
        3 уже удалились, 4 и 5 - еще нет. Запрос их прочитает, т.е.
        половину пользовательской транзакции.

\end{enumerate}

\texttt{Vshard}-у следует предоставить собственное решение для выполнения
map-запроса по репликам, который не будет дублировать и терять бакеты.

\subsection{Текущая работа Map запроса по мастерам (map\_callrw)}

\texttt{vshard.router.map\_callrw} принимает в качестве аргумента функцию,
которая вызывается на мастерах в кластере с гарантией, что в случае успешного
выполнения она была выполнена при доступности всех бакетов для чтения и записи.
Под консистентностью в рамках Map-Reduce понимается, что все данные были
доступны и не перемещались в процессе выполнения запросов Map. Чтобы сохранить
консистентность, добавлена третья стадия - Ref. Таким образом, алгоритм на
самом деле называется Ref-Map-Reduce.

Ref-ы отправляются до этапа Map, чтобы закрепить бакеты на репликасетах и
гарантировать, что они не будут перемещаться до завершения стадии Map. Если
хотя бы один Ref не удалось проставить, то пользователю возвращается ошибка.

Если все Ref-ы успешно завершены, начинается рассылка запросов Map. Эти запросы
выполняют пользовательскую функцию и после удаляют Ref-ы, чтобы снова разрешить
ребалансировку. Рассылка запросов Map производится параллельно, ко всем
необходимым мастерам.

На уровне storage существуют дополнительные механизмы, чтобы гарантировать,
что Map-Reduce не блокирует ребалансировку навсегда и наоборот: что
ребалансировка не заблокирует полностью Map-Reduce. Это конкурирующие процессы.
В vshard есть опции: \texttt{sched\_ref\_quota}, \texttt{sched\_move\_quota}.
Если \texttt{move\_quota = 2} и \texttt{ref\_quota = 15}, то это означает, что
максимум 2 бакета могут переехать, если есть запросы на ref. И наоборот,
максимум 15 ref может быть сделано, если нужно перевозить бакет.

Подробней про работу \texttt{map\_callrw} можно прочитать в соответствующем
дизайн-документе \cite{MapCallrwRfc}.

Аргументы vshard.router.map\_callrw:

\begin{itemize}
\item \texttt{func}: Имя вызываемой функции.
\item \texttt{args}: Аргументы функции, переданные в формате netbox (в виде
    массива).
\item \texttt{opts.timeout}: Таймаут в секундах. Учтите, что Ref-ы могут
    оставаться на хранилищах в течение всего этого таймаута, если что-то пойдет
        не так, например, возникнут сетевые проблемы. Поэтому лучше не
        использовать значение больше, чем необходимо.
\item \texttt{opts.return\_raw}: true/false. Если указано, возвращаемые
    значения не декодируются в нативные объекты Lua и остаются упакованными как
        объект msgpack (см. модуль msgpack). По умолчанию все значения
        декодируются. Это может быть нежелательно, если возвращаемые значения
        будут сразу пересылаться по сети.
\item \texttt{opts.bucket\_ids}: Массив ID бакетов, которые должны быть
    охвачены Map-Reduce. Если она указана, Map-Reduce выполняется только на
        мастерах, содержащих хотя бы один из этих бакетов. В противном случае
        Map-Reduce выполняется на всех мастерах кластера.
\end{itemize}

\subsection{Дизайн}

В map-запросах все фазы Ref-Map выполняются на одной и той же реплике:

\begin{itemize}
\item \texttt{map\_callrw} -- выполняется только на мастере. Реализован в
    текущей версии.

\item \texttt{map\_callro} -- реплика выбирается на основе весов в конфигурации
    роутера (чем меньше вес, тем реплика приоритетнее). Такой репликой может
        являться и мастер (при использовании весов по умолчанию мастер является
        наиболее приоритетной репликой). Выбор реплики не является постоянным и
        может временно меняться, если самая приоритетная реплика недоступна.

\item \texttt{map\_callre} -- предпочтение отдается не мастеру (но все еще
    может быть выполнена на мастере как последняя мера, если все реплики не
        ответили).

\item \texttt{map\_callbro} -- round-robin балансировка по всем узлам в
    репликасете, может быть как мастер, так и реплика.

\item \texttt{map\_callbre} -- round-robin балансировка, но с предпочтением
    пропускать мастера.
\end{itemize}

\subsubsection{Ref стадия запроса}

Для map запросов по репликам применяются те же ref, что используются сейчас
для мастеров, ref полностью локальный (если узел не мастер, то запрос на
мастера не совершается). Ref происходит до запроса ко всем репликам, в
отдельную фазу, как и для \texttt{map\_callrw}. Ref так же как и в случае c
\texttt{map\_callrw} дожидается, чтобы все бакеты на узле были
\texttt{ACTIVE} или \texttt{PINNED}. Если не получается взять ref (идет
ребалансировка), возвращается ошибка, ни один map запрос не будет выполнен.
Если ref успешно создан на всех репликах, выполняется пользовательская функция,
после чего ref удаляется.

Ref блокирует все бакеты на узле, пока висит Ref, ребалансировка невозможна.

\subsubsection{Map запросы и ребалансировка бакетов}

Мастеру приходит запрос на перевоз N бакетов. Он собирает эти N бакетов и перед
началом отправления делает \texttt{sched.move\_start} локально. В случае успеха
мастер переводит бакеты в состояние \texttt{SENDING}. Далее, данные не
отправляются/получаются, пока мы не получим на это разрешение всех реплик в
нашем репликасете. Мастер дожидается на каждой из реплик выполнение функции
\texttt{bucket\_move\_prepare\_replica(timeout, vclock\_of\_first\_bucket)}, где
\texttt{vclock} -- это vclock мастера после того, как был изменен статус
первого измененного бакета. Эта функция выполняется на реплике следующим
образом:

\begin{itemize}
\item Вызывает \texttt{sched.move\_start} чтобы создать конкуренцию за ресурсы
    на репликах и обеспечить заданное пользователем (с помощью
        \texttt{sched\_*\_quota} опций) распределение между перевозом бакетов и
        ref запросами.
\item Дожидается, чтобы реплика достигла необходимого vclock. Т.е. получила
    хотя бы один \texttt{SENDING} бакет. Этого достаточно, чтобы новые ref не
        могли быть созданы и ждать, пока будут пересланы все, смысла ждать нет.
\item Вызывает \texttt{sched.move\_end(1)}. Отныне новые ref не могут быть
    созданы, так как бакет перемещается.
\end{itemize}

Если хотя бы одна реплика ответила ошибкой, бакеты не будут посылаться, ошибка
отправления бакета. Не меняем состояние бакета, в случае ошибок этим занимается
recovery (не забыть разбудить в случае ошибки) и gc. Если все реплики смогли
подтвердить перевоз бакетов, то обращаемся к мастеру, на который данные будут
отправляться: \texttt{bucket\_recv\_prepare\_master(timeout, buckets)}. Он
переводит бакеты в состояние \texttt{RECEIVING}, делает вызов по репликам
\texttt{bucket\_move\_prepare\_replica}, чтобы убедиться, что новые ref не смогут
быть созданы и дождаться окончания \texttt{map\_callro}. Если успешно,
возвращает ок.

Вся работа мастера, описанная выше объединяется в функцию
\texttt{bucket\_send\_prepare\_master(timeout, buckets)}.

Таким образом на посылку/получение одного набора бакетов от конкретного
репликасета) нужно будет совершить только по одному запросу на каждую из
реплик. Дальше при пересылке бакетов работают рабочие процессы, как обычно (но
не переводя в состояние \texttt{SENDING} и то, что описано выше).

Чтобы предотвратить одновременную посылку одного и того же бакета (например,
когда ребалансировщик пытается послать и пользователь вызывает
\texttt{bucket\_send}), \texttt{M.rebalancer\_transfering\_buckets[bid] =
'preparing'} выполняется для каждого из бакетов перед локальным
\texttt{sched\_move\_start} на мастере (в самом начале
\texttt{bucket\_send\_prepare\_master}). Затем когда рабочий процесс начинает
работу он проверяет, что статус бакета \texttt{preparing} (можно было бы
добавить статус \texttt{prepared} после того, как дождались разрешения реплик,
но выглядит бессмысленно, мы не должны отдавать работу в рабочие процессы, если
бакет не разрешили пересылать), меняет его в \texttt{transfering}. На
принимающей стороне состояние бакета меняется на \texttt{preparing} в
\texttt{bucket\_recv\_prepare\_master} (recovery должен проверять эти статусы и
подчищать их, если на отправляющей стороне
\texttt{rebalancer\_transfering\_buckets} в \texttt{nil}). На стадии
\texttt{bucket\_recv} состояние бакета меняется на \texttt{transfering}. По
умолчанию \texttt{M.rebalancer\_transfering\_buckets[bid] = nil}.

Если \texttt{bucket\_recv} видит, что бакета не существует, тогда он вызывает
\texttt{bucket\_move\_prepare\_replica}. Необходимо, чтобы ребалансировка
работала, если отправитель на старой версии, а получатель на новой. Если
отправитель на новой версии, то пересылка бакета будет падать на стадии
\texttt{bucket\_recv\_prepare\_master}.

\subsection{Последующе действия}

Описанное в данном пункте будет реализовано только по запросу от пользователей.
В изначальную имплементацию map запросов этот пункт не входит.

Во все \texttt{map\_*} запросы добавляется новая опция
\texttt{consistency\_coverage\_mode}. При обоих режимах выполняется попытка
остановить ребалансировку!

\begin{itemize}
    \item \texttt{consistency\_coverage\_mode = 'full'}. Запрос будет выполнен
        везде или не будет выполнен вовсе, запрос гарантированно будет без
        дублирования данных, без их потери, т.е. будет выполнен на каждом
        бакете и ровно один раз.

    \texttt{map\_*} запрос выполняется в две фазы: Ref - попытка приостановить
        ребалансировку на всех узлах, Map - выполнение пользовательской
        функции. Необходим в первую очередь для \texttt{map\_callrw}. Если
        ошибка на стадии Ref - возврат ошибки пользователю, функция не будет
        исполнена нигде.

    \item \texttt{consistency\_coverage\_mode = 'accidental'}. Запрос будет
        выполнен там, где удастся остановить ребалансировку. Там, где он
        выполнится, дублирования данных не будет, т.е. функция не будет
        исполнена дважды на одном и том же бакете, "not more than once". Но
        данные в рамках кластера могут быть частично прочитаны и записаны, нет
        гарантии "at least once" для каждого из бакетов.

    \texttt{map\_*} запрос тоже выполняется в две фазы. Единственное отличие от
        \texttt{full} режима: если на стадии Ref не удалось выполнится на всех
        узлах, то стадия Map выполняется на тех узлах, где Ref прошел
        успешно. Всегда возвращается таблица типа \texttt{\{<replicaset\_uuid>
        = <returned\_value>, ...\}}.

    Полезен при \texttt{map\_*} запросах на чтение.
\end{itemize}

Первый режим необходим для работы \texttt{map\_callrw}. Необходимо
гарантировать, что запрос выполнен на всех репликасетах, а не на их части. Ибо
если мы записали что-то только на части репликасетов, то это беда, частичное
обновление данных. Однако, если пользователь, например, читает только с
мастеров (например, для обеспечения максимальной согласованности данных), то
имеет смысл использовать \texttt{accidental}.

Второй режим будет очень полезен для \texttt{map\_callro}, так как в
большинстве случаев лучше прочитать хоть что-то (например делаем select по
спейсу), чем не читать вообще ничего. Однако если данные невозможно
использовать частично, \texttt{consistency\_coverage\_mode} все равно должен
стоять в \texttt{full}.

Режим \texttt{full} существует уже сейчас в \texttt{map\_callrw}. Будет
добавлен только \texttt{accidental}. Для map запросов по репликам в изначальной
имплементации реализовываем только \texttt{full} режим, \texttt{accidental}
добавляется в задачу.

\subsection{Рассмотренные альтернативы}

\subsubsection*{Вызывать sched.move\_start на репликах перед изменением состояния бакета}

Была идея, чтобы перед отправлением/получением бакета, до изменения состояния
бакета совершать \texttt{sched.move\_start}. Только если сумели дождаться 0
ref везде, можно начинать ребалансировку, производится перевод бакета в
нужное состояние на мастере, бакет перевозится. Реплика автоматически зовет
\texttt{sched.move\_end} в on\_replace триггере когда получает по репликации
\texttt{replace} в \texttt{\_bucket}. Если же где-то есть ref и мы не смогли
дождаться, то отменяем все с помощью \texttt{sched.move\_end} с мастера.

Однако тут мы наблюдаем стандартную проблему 2PC. Если мы не используем
таймаут, после которого автоматически зовется \texttt{sched.move\_end}, то,
если мастер падает сразу после вызова \texttt{sched.move\_start}, реплика
навсегда заблокирована. Если мы используем таймауты, то можем получить
неконсистентные данные, так как таймаут может отработать раньше, чем придет
replace в \_bucket, будет создан ref для map\_call, а потом к нам долетит
replace: во время исполнения функции пользователя мы нарушили гарантии того,
что состояние бакетов не будет меняться.

\subsection*{Включение map\_callro только по опции в конфигурации}

Предполагается, что \texttt{map\_call} с \texttt{mode = 'read'} будет
использоваться редко, а потому нет смысла усложнять ребалансировку бакетов для
пользователей, которые не пользуются данной функцией. Для этого добавляется
новая опция конфигурирования vshard (будет проброшена в cartridge):
\texttt{enable\_read\_map\_calls} (name subject to change) - опция может быть
задана на уровне репликасета или глобально в конфигурации (для всех
репликасетов сразу). Включает проверки собственного репликасета на ref-ы при
ребалансировке (пересылке бакетов и recovery). По умолчанию равна false.

Когда эта опция \texttt{false} на роутере, роутер будет сразу же возвращать
ошибку \texttt{UNSUPPORTED}, без запросов к репликам, при попытке использовать
любой \texttt{map\_call} с \texttt{mode = 'read'}. Если же на роутере она
\texttt{true}, а на storage \texttt{false}, то реплика будет возвращать эту
ошибку когда роутер делает запрос. Чтобы \texttt{map\_call} по репликам работал,
опция должна быть включена везде.

Приняли решение отказаться от этой опции, чтобы всегда обеспечивать
максимальные проверки при ребалансировке.
